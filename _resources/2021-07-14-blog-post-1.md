---
title: 'Chapter 3 Finite Markov Decision Processes'
date: 2021-07-14
permalink: /posts/2013/08/blog-post-1/
tags:
  - RL
---
### The Agent-Environment Interface

1. **Agent:** leaner & decision-maker

   1. GOAL: <u>*maximize the total amount of reward*</u> it receives over the long run.
   2. known and controllable

2. **Enviroment:** The things agents interact with, comprising everything outside the agent.

   - Note: *the boundary between agent and enviroment is typically not the smae as the physical boundary of a robot's or an animal's body.*
   - *The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.*

3. **MDP Framework:**

   <img src="../images/book/Screen%20Shot%202021-07-11%20at%2012.48.57%20PM.png" style="zoom:67%;" />

   - Agent and environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,\dots$.
   - At time $t$,
     - agent receives some representation of the enviroment's state $S_t\in\mathcal{S}$.
     - policy: a mapping from states to prob- abilities of selecting each possible action
       - $\pi_t(a|s)$: the probability that $A_t=a$ if $S_t=s$.
     - based on $S_t$, selects an action $A_t\in\mathcal{A}(S_t)$
     - in the next time $t+1$,  agent will receive a numerical reward $R_{t+1}\in\mathcal{R}$ and next state $S_{t+1}$
     - trajectory: $S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\ldots$
   - **State & Action:**
     - Action: any decisions we want to learn to how to make
     - State: Anything we can know that might be useful in making actions.
   - **Reward:**
     - define the agent's goal
     - be external to the agent
     - but often assume the agent knows quite a bit about how its rewards are computed as a fuction of its actions and states in which they are taken.

4. **dynamics** of the MDP: [4-argument dynamics function $p$]

   1. *completely* characterize the <u>enviroment</u>'s dynamics
   2. In Finite MDP: the random variables $R_t$ and $S_t$  have well defined discrete probability distributions dependent *only on the preceding state and action*, $S_{t-1}$ and $A_{t-1}$.
   3. $p\left(s^{\prime}, r \mid s, a\right) \doteq \operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \mid S_{t-1}=s, A_{t-1}=a\right\}$
   4. $\sum_{s^{\prime} \in \mathcal{S}} \sum_{r \in \mathcal{R}} p\left(s^{\prime}, r \mid s, a\right)=1$, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$

5. state-transition probabilities: [3-argument dynamics function $p$]

   - $p\left(s^{\prime} \mid s, a\right) \doteq \operatorname{Pr}\left\{S_{t}=s^{\prime} \mid S_{t-1}=s, A_{t-1}=a\right\}=\sum_{r \in \mathcal{R}} p\left(s^{\prime}, r \mid s, a\right)$

6. **expected rewards for state-action pairs**: [2-arugment function $r$]

   - $r(s, a) \doteq \mathbb{E}\left[R_{t} \mid S_{t-1}=s, A_{t-1}=a\right]=\sum_{r \in \mathcal{R}} r \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime}, r \mid s, a\right)$

7. expected rewards for state-action-next-state triples: [3-argument function $r$]

   - $r\left(s, a, s^{\prime}\right) \doteq \mathbb{E}\left[R_{t} \mid S_{t-1}=s, A_{t-1}=a, S_{t}=s^{\prime}\right]=\sum_{r \in \mathcal{R}} r \frac{p\left(s^{\prime}, r \mid s, a\right)}{p\left(s^{\prime} \mid s, a\right)}$
     - that's because $p(r|s,a,s') =\dfrac{p\left(s^{\prime}, r \mid s, a\right)}{p\left(s^{\prime} \mid s, a\right)} $

8. Markov Property:

   - the state includes information about all aspects of the past agent-enviroment interaction that make a difference for the future.





### Goals and Rewards

1. Reward Hypothesis:

   *That all of what we mean by **goals** and purposes can be well thought of as the maximization of the expected value of the cumulative sum of **a received scalar signal (called reward)**.*

2. **The reward signal is your way of comminicating to the agent <u>what you want achieved</u>, NOT how you want it achieved.**



### Returns and Episodes

1. **Return**: some specific function of the reward sequence.

2. **Episodic tasks:** 

   1. Tasks with episodes [the subsequences that the agent-enviroment interaction can break naturally into]

   2. Each episode ends in a special **terminal state**, followed by a *reset* to a standard starting state or to a sample from a standard distribution of starting states.

      1. The next episode begins independently of how the previous one ended.
      2. The time of termination, $T$, is a random variable that normally varies from episode to episode.

   3. Return: 
      $$
      G_t = R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_{T}
      $$
      

3. **Continuing tasks:**

   1. When the agent-environment interaction does not break naturally into identifiable episodes, but **goes on continually without limit.**

   2. **Discounted return**

      - $$
        		
        				G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}  
        				
        $$

        

        - $\gamma \in [0,1]$: discount rate.
          - $\gamma<1$:  $G_t$ has a finite value as long as the reward sequence $\{R_k\}$ is bounded.
          - $\gamma = 0$: myopic, only considering to maximize immediate rewards.
          - $\gamma = 1$: farsighted.
        - $G_t = R_{t+1}+\gamma G_{t+1}$
          - work for all time steps $t<T$, even if termination occurs at $t+1$, provided we define $G_T = 0$.



### Unified Notation for Episodic and Continuing Tasks

1. For episodic tasks:

   - $S_{t,i}$: the state representation at time $t$ of episode $i$. (and similaly for $A_{t,i}$, $R_{t,i}$,  $\pi_{t,i}$, $T_i$....)

   - BUT, actually, we almost never have to distinguish between different episodes.

   - ----> we write $S_t$ to refer to $S_{t,i}$ and so on.

   - Add **absorbing state** that transitions only to itself and that generates only rewards of zero. like

     <img src="../images/book/Screen%20Shot%202021-07-12%20at%202.02.13%20PM.png" style="zoom: 50%;" />

2. Unified Return:
   $$
   G_t = \sum_{k=t+1}^T \gamma^{k-t-1} R_k
   $$
   

   - episodic task:    $\gamma = 1$
   - contiuning task:    $T = \infty$



### Policies and Value Functions

1. **Policy**:

   - a mapping from states to probabilites of selecting each possible action.
     - if the agent is following policy $\pi$ at time $t$, then $\pi(a|s)$ is the probability that $A_t = a$ if $S_t = s$.

2. **Value function** of a state $s$ under a policy $\pi$ :
   $$
   v_{\pi}(s): = \mathbb{E}_\pi [G_t|S_t=s] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s\right], \quad \forall s\in\mathcal{S}
   $$

   - is the expected return when starting in $s$ and following $\pi$ thereafter.

   - is the **state-value function for policy $\pi$ .**

   - **Bellman equation for $v_\pi$**
     $$
     v_\pi (s) = \sum_a \pi(a|s) \sum_{s',r'} p(s',r'|s,a) [r+\gamma v_{\pi}(s')], \quad\forall s\in\mathcal{S}
     $$
     <img src="./../images/book/Screen%20Shot%202021-07-12%20at%203.03.06%20PM.png" style="zoom:50%;" />

     - The bellman equation averages over all the possibilites, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.
     - The value function $v_\pi$ is the unique solution to its <u> Bellman equation</u>. 

3. **Action-value function for policy $\pi$**
   $$
   q_{\pi}(s,a): = \mathbb{E}_\pi [G_t|S_t=s,A_t= a] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s,A_t=a\right], \quad \forall s\in\mathcal{S}
   $$

   - is the expected return when starting in $s$, taking the action $a$ and following $\pi$ thereafter.

4. The value functions $v_{\pi}$ and $q_{\pi}$ can be estimated from experience. 

   - **Monte Carlo method**: averaging over many random samples of actual returns.
   - maintain $v_\pi$ and $q_\pi$ as parameterized functions (with fewer parameters than states) and adjust the parameters to better match the observed returns.

   

### Optimal Policies and Optimal Value Functions

1. **Optimal state-value function**: $v_*(s) = \max_\pi v_\pi(s)$, for all $s\in\mathcal{S}$.

2. **Optimal action-value function**: $q_*(s,a) = \max_{\pi} q_{\pi}(s,a)$, for all $s\in\mathcal{S}$ and $a\in\mathcal{A(s)}$

3. Note：
   $$
   q_*(s,a) = \mathbb{E} [R_{t+1} +\gamma v_*(S_{t+1})|S_t = s,A_t = a]
   $$
   
4. Optimal policy: $\pi^* = \arg\max_{\pi} v_{\pi}(s)$ 

5. $v_*(s)$ and $q_*(s,a)$ are <u>unique</u> for a given MDP.

6. **Bellman optimality equation for $v_*$ **
   $$
   \begin{aligned}
   v_*(s) & = \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t = s,A_t=a]\\
   & = \max_a\sum_{s',r} p(s',r|s,a)[r+\gamma v_*(s')]
   \end{aligned}
   $$

   - In finite MDP, the above equation has a unique solution. (since if there are $n$ states, then there are $n$ equations in $n$ unknowns.)
   - How to find optimal policies, by means of $v_*$: 
     - Once we have $v_*$, then any policy that assigns nonzero probability only to these actions [at which the maximum is obtained in the Bellman optimality equation] is an optimal policy. 
     - Look like greedy w.r.t. the optimal evaluation function $v_*$.
     - BUT it is long-term sense due to the beauty $v_*$

6. **Bellman optimality equation for $q_*$** 
   $$
   \begin{aligned}
   q_*(s,a) & = \mathbb{E}[R_{t+1}+\gamma \max_{a'} q_*(S_{t+1},a')|S_t = s,A_t=a]\\
   & = \sum_{s',r} p(s',r|s,a)[r+\gamma \max_{a'} q_*(s',a')]
   \end{aligned}
   $$

   - How to find optimal policies, by means of $ q_*$:  
     - The action-value function effectively <u>caches</u> the results of all one-step-ahead searches. 

   <img src="../images/book/Screen%20Shot%202021-07-12%20at%204.01.48%20PM.png" style="zoom:50%;" />

7. Explicitly solving the Bellman optimality equation provides one route to finding an optimal policy, and thus to solving the reinforcement learning problem. **However, this solution is rarely directly useful.**



### Optimality and Approximation

1. Key property that distinguishes RL from other approaches to approximately solving MDPs.
   - The online nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. 

### Summary

- Reinforcement learning is about learning from interaction how to behave in order to achieve a goal.
-  A finite MDP is an MDP with finite state, action, and (as we formulate it here) reward sets. 
- Tasks are defined by: 
  - the **actions** are the choices made by the agent; 
  - the **states** are the basis for making the choices; 
  - the **rewards** are the basis for evaluating the choices.
- **Policy**:  is a stochastic rule by which the agent selects actions as a function of states.
- **Return:** is the function of future rewards that the agent seeks to maximize (in expected value).
- Policy’s **value functions** ($v_\pi$  and $q_\pi$ ): assign to each state, or state–action pair, the expected return from that state, or state–action pair, given that the agent uses the policy.
- **Optimal value functions** ($v_*$and $q_*$) assign to each state, or state–action pair, the *largest expected return* achievable by any policy
- **Optimal policy**: a policy whose value functions are optimal 

